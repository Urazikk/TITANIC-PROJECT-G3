{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data\n",
    "\n",
    "First, let's import the necessary libraries and load our training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12) (418, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Imports & load\n",
    "import numpy as np, pandas as pd, re, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "# libs ML\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Optional boosters\n",
    "HAS_XGB = HAS_LGBM = HAS_CAT = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    HAS_CAT = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Load\n",
    "train_raw = pd.read_csv(\"data/train.csv\")\n",
    "test_raw  = pd.read_csv(\"data/test.csv\")\n",
    "print(train_raw.shape, test_raw.shape)\n",
    "train_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now, let's explore the data to understand its structure, find patterns, and identify missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (891, 24) (418, 24)\n"
     ]
    }
   ],
   "source": [
    "# 2) Feature engineering + OOF Target Encoding (anti-fuite)\n",
    "\n",
    "def build_features(df: pd.DataFrame, ref_train: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Title\n",
    "    out[\"Title\"] = out[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\", expand=False)\n",
    "    out[\"Title\"] = out[\"Title\"].replace({\"Mlle\":\"Miss\",\"Ms\":\"Miss\",\"Mme\":\"Mrs\"})\n",
    "    rare = ref_train[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\", expand=False).replace({\"Mlle\":\"Miss\",\"Ms\":\"Miss\",\"Mme\":\"Mrs\"}).value_counts()\n",
    "    out[\"Title\"] = out[\"Title\"].where(~out[\"Title\"].isin(rare[rare<10].index), \"Rare\")\n",
    "\n",
    "    # Embarked (mode du train)\n",
    "    emb_mode = ref_train[\"Embarked\"].mode().iloc[0]\n",
    "    out[\"Embarked\"] = out[\"Embarked\"].fillna(emb_mode)\n",
    "\n",
    "    # Fare imputé par (Pclass, Embarked)\n",
    "    fare_by = ref_train.assign(Embarked=ref_train[\"Embarked\"].fillna(emb_mode))\\\n",
    "                       .groupby([\"Pclass\",\"Embarked\"])[\"Fare\"].median()\n",
    "    out[\"Fare\"] = out.apply(lambda r: fare_by.loc[(r[\"Pclass\"], r[\"Embarked\"])] if pd.isna(r[\"Fare\"]) else r[\"Fare\"], axis=1)\n",
    "\n",
    "    # Family\n",
    "    out[\"FamilySize\"]    = out[\"SibSp\"].fillna(0) + out[\"Parch\"].fillna(0) + 1\n",
    "    out[\"IsAlone\"]       = (out[\"FamilySize\"]==1).astype(int)\n",
    "    out[\"FarePerPerson\"] = out[\"Fare\"] / out[\"FamilySize\"].clip(lower=1)\n",
    "\n",
    "    # Surname/FamilyID\n",
    "    out[\"Surname\"]  = out[\"Name\"].str.extract(r\"^([^,]+),\", expand=False).str.strip()\n",
    "    out[\"FamilyID\"] = out[\"Surname\"].fillna(\"UNK\") + \"_\" + out[\"FamilySize\"].astype(int).astype(str)\n",
    "    # compresser les toutes petites familles pour limiter la variance\n",
    "    # (on gardera quand même une TE dessus)\n",
    "    # pas de rare handling ici, la TE lisse déjà.\n",
    "\n",
    "    # Ticket/Cabin\n",
    "    out[\"TicketClean\"] = out[\"Ticket\"].astype(str).str.replace(r\"\\s+\", \"\", regex=True)\n",
    "    out[\"TicketPrefix\"] = (out[\"Ticket\"].astype(str)\n",
    "                           .str.replace(r\"[\\./]\", \" \", regex=True)\n",
    "                           .str.extract(r\"^([A-Za-z]+)\", expand=False)\n",
    "                           .fillna(\"NONE\"))\n",
    "    out[\"CabinDeck\"] = out[\"Cabin\"].astype(str).str[0].where(out[\"Cabin\"].notna(), \"U\")\n",
    "    out[\"CabinCount\"] = out[\"Cabin\"].fillna(\"\").astype(str).apply(lambda s: 0 if s==\"\" else len(re.split(r\"\\s+\", s.strip())))\n",
    "\n",
    "    # Heuristiques\n",
    "    out[\"Child\"]  = (out[\"Age\"] < 14).astype(float)\n",
    "    out[\"Mother\"] = ((out[\"Sex\"]==\"female\") & (out[\"Parch\"]>0)).astype(int)\n",
    "\n",
    "    # Impute Age par (Title,Pclass,Sex)\n",
    "    age_group = ref_train.assign(\n",
    "        Title=ref_train[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\", expand=False).replace({\"Mlle\":\"Miss\",\"Ms\":\"Miss\",\"Mme\":\"Mrs\"}),\n",
    "        Embarked=ref_train[\"Embarked\"].fillna(emb_mode)\n",
    "    ).groupby([\"Title\",\"Pclass\",\"Sex\"])[\"Age\"].median()\n",
    "\n",
    "    def impute_age(row):\n",
    "        if pd.isna(row[\"Age\"]):\n",
    "            key = (row[\"Title\"], row[\"Pclass\"], row[\"Sex\"])\n",
    "            return age_group.loc[key] if key in age_group.index else ref_train[\"Age\"].median()\n",
    "        return row[\"Age\"]\n",
    "    out[\"Age\"] = out.apply(impute_age, axis=1)\n",
    "\n",
    "    # Logs stables\n",
    "    out[\"Fare_log\"]          = np.log1p(out[\"Fare\"])\n",
    "    out[\"FarePerPerson_log\"] = np.log1p(out[\"FarePerPerson\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "train_fe = build_features(train_raw, train_raw)\n",
    "test_fe  = build_features(test_raw,  train_raw)\n",
    "\n",
    "# Group size par Ticket (calculé sur full concat sans Survived)\n",
    "full_tmp = pd.concat([train_fe.drop(columns=[\"Survived\"]), test_fe], axis=0, ignore_index=True)\n",
    "ticket_counts = full_tmp[\"TicketClean\"].value_counts()\n",
    "train_fe[\"TicketGroupSize\"] = train_fe[\"TicketClean\"].map(ticket_counts).fillna(1)\n",
    "test_fe[\"TicketGroupSize\"]  = test_fe[\"TicketClean\"].map(ticket_counts).fillna(1)\n",
    "\n",
    "# ---------- OOF Target Encoding (anti-fuite) ----------\n",
    "def oof_target_encode(train_df, test_df, col, target=\"Survived\", n_splits=5, seed=42, smoothing=30):\n",
    "    global_mean = train_df[target].mean()\n",
    "    enc_train = pd.Series(np.zeros(len(train_df)), index=train_df.index, dtype=float)\n",
    "    enc_test  = pd.Series(np.zeros(len(test_df)), dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for tr_idx, va_idx in kf.split(train_df, train_df[target]):\n",
    "        tr, va = train_df.iloc[tr_idx], train_df.iloc[va_idx]\n",
    "        stats = tr.groupby(col)[target].agg([\"mean\",\"count\"])\n",
    "        stats[\"te\"] = (stats[\"count\"]*stats[\"mean\"] + smoothing*global_mean) / (stats[\"count\"] + smoothing)\n",
    "        enc_train.iloc[va_idx] = va[col].map(stats[\"te\"]).fillna(global_mean).values\n",
    "        enc_test += test_df[col].map(stats[\"te\"]).fillna(global_mean) / n_splits\n",
    "    return enc_train, enc_test\n",
    "\n",
    "# Colonnes high-card + composites (agressif mais OOF only)\n",
    "hi_cols = [\"FamilyID\",\"TicketClean\",\"Surname\"]\n",
    "for c in hi_cols:\n",
    "    tr_enc, te_enc = oof_target_encode(train_fe, test_fe, c)\n",
    "    train_fe[f\"TE_{c}\"] = tr_enc.values\n",
    "    test_fe[f\"TE_{c}\"]  = te_enc.values\n",
    "\n",
    "def add_composite_te(train_df, test_df, cols):\n",
    "    key = \"__\".join(cols)\n",
    "    tr_tmp = train_df.copy(); te_tmp = test_df.copy()\n",
    "    tr_tmp[key] = tr_tmp[cols].astype(str).agg(\"_\".join, axis=1)\n",
    "    te_tmp[key] = te_tmp[cols].astype(str).agg(\"_\".join, axis=1)\n",
    "    tr_enc, te_enc = oof_target_encode(tr_tmp, te_tmp, key)\n",
    "    train_df[f\"TE_{key}\"] = tr_enc.values\n",
    "    test_df[f\"TE_{key}\"]  = te_enc.values\n",
    "\n",
    "add_composite_te(train_fe, test_fe, [\"TicketClean\",\"Sex\"])\n",
    "add_composite_te(train_fe, test_fe, [\"Surname\",\"Sex\"])\n",
    "add_composite_te(train_fe, test_fe, [\"FamilyID\",\"Pclass\"])\n",
    "\n",
    "# Features\n",
    "y = train_fe[\"Survived\"].astype(int)\n",
    "num_cols = [\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"FamilySize\",\"IsAlone\",\"Fare_log\",\"FarePerPerson_log\",\n",
    "            \"CabinCount\",\"Child\",\"Mother\",\"TicketGroupSize\",\n",
    "            \"TE_FamilyID\",\"TE_TicketClean\",\"TE_Surname\",\"TE_TicketClean__Sex\",\"TE_Surname__Sex\",\"TE_FamilyID__Pclass\"]\n",
    "cat_cols = [\"Pclass\",\"Sex\",\"Embarked\",\"Title\",\"TicketPrefix\",\"CabinDeck\"]\n",
    "\n",
    "X = train_fe[num_cols + cat_cols].copy()\n",
    "X_test = test_fe[num_cols + cat_cols].copy()\n",
    "print(\"Shapes:\", X.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Feature Engineering\n",
    "\n",
    "Based on our EDA, we'll clean the data by handling missing values and create new features to improve our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hgb OOF acc @0.5: 0.8541 ± 0.0094\n",
      "gb OOF acc @0.5: 0.8361 ± 0.0245\n",
      "rf OOF acc @0.5: 0.8406 ± 0.0228\n",
      "et OOF acc @0.5: 0.8271 ± 0.0267\n",
      "svc OOF acc @0.5: 0.8552 ± 0.0190\n",
      "lr OOF acc @0.5: 0.8372 ± 0.0144\n",
      "STACK OOF calibrated — best_thr: 0.460 | acc: 0.8575\n",
      "group (male, 1) thr=0.460\n",
      "group (male, 2) thr=0.460\n",
      "group (male, 3) thr=0.460\n",
      "group (female, 1) thr=0.460\n",
      "group (female, 2) thr=0.460\n",
      "group (female, 3) thr=0.460\n"
     ]
    }
   ],
   "source": [
    "# 3) Stacking niveau 1 (OOF), méta LR + calibration + seuils\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Preprocess commun\n",
    "num_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())])\n",
    "cat_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                   (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "preprocess = ColumnTransformer([(\"num\", num_tf, num_cols), (\"cat\", cat_tf, cat_cols)], verbose_feature_names_out=False)\n",
    "\n",
    "# Base models (beaucoup de diversité)\n",
    "base_models = []\n",
    "if HAS_CAT:\n",
    "    base_models.append((\"cat\", CatBoostClassifier(iterations=1800, learning_rate=0.03, depth=5, l2_leaf_reg=3.0,\n",
    "                                                  loss_function=\"Logloss\", random_seed=42, verbose=False)))\n",
    "if HAS_LGBM:\n",
    "    base_models.append((\"lgbm\", LGBMClassifier(n_estimators=1400, learning_rate=0.03, num_leaves=31,\n",
    "                                               subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "                                               random_state=42, n_jobs=-1)))\n",
    "if HAS_XGB:\n",
    "    base_models.append((\"xgb\", XGBClassifier(n_estimators=1200, max_depth=3, learning_rate=0.03,\n",
    "                                             subsample=0.9, colsample_bytree=0.9, reg_lambda=1.1, reg_alpha=0.02,\n",
    "                                             min_child_weight=1.0, objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "                                             random_state=42, n_jobs=-1)))\n",
    "# scikit grand classique\n",
    "base_models += [\n",
    "    (\"hgb\", HistGradientBoostingClassifier(learning_rate=0.05, max_depth=3, max_leaf_nodes=31,\n",
    "                                           l2_regularization=1.0, min_samples_leaf=12, early_stopping=False, random_state=42)),\n",
    "    (\"gb\", GradientBoostingClassifier(n_estimators=800, learning_rate=0.05, max_depth=3, subsample=0.9, random_state=42)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=1200, n_jobs=-1, random_state=42)),\n",
    "    (\"et\", ExtraTreesClassifier(n_estimators=1200, n_jobs=-1, random_state=42)),\n",
    "    (\"svc\", SVC(C=1.2, kernel=\"rbf\", gamma=\"scale\", probability=True, random_state=42)),\n",
    "    (\"lr\", LogisticRegression(max_iter=8000, C=1.5))\n",
    "]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_stack = np.zeros((len(X), len(base_models))); test_stack = np.zeros((len(X_test), len(base_models)))\n",
    "\n",
    "for m_idx, (name, mdl) in enumerate(base_models):\n",
    "    oof_proba = np.zeros(len(X)); test_f = np.zeros((len(X_test), skf.get_n_splits()))\n",
    "    scores = []\n",
    "    for k, (tr, va) in enumerate(skf.split(X, y)):\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"model\", clone(mdl))])\n",
    "        pipe.fit(X.iloc[tr], y.iloc[tr])\n",
    "        p_va = pipe.predict_proba(X.iloc[va])[:,1]; oof_proba[va] = p_va\n",
    "        scores.append(accuracy_score(y.iloc[va], (p_va>=0.5).astype(int)))\n",
    "        test_f[:,k] = pipe.predict_proba(X_test)[:,1]\n",
    "    oof_stack[:, m_idx] = oof_proba; test_stack[:, m_idx] = test_f.mean(axis=1)\n",
    "    print(f\"{name} OOF acc @0.5: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
    "\n",
    "# Méta LR (avec CV) sur les sorties OOF\n",
    "scaler_meta = StandardScaler(); X_meta = scaler_meta.fit_transform(oof_stack)\n",
    "meta = LogisticRegressionCV(Cs=10, cv=5, scoring=\"accuracy\", max_iter=20000, n_jobs=-1, refit=True)\n",
    "meta.fit(X_meta, y)\n",
    "\n",
    "# Calibration isotone apprise sur OOF\n",
    "oof_meta_raw = meta.predict_proba(X_meta)[:,1]\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(oof_meta_raw, y.values)\n",
    "oof_meta = iso.transform(oof_meta_raw)\n",
    "\n",
    "# Seuil global (petite plage autour 0.5 pour éviter la dérive)\n",
    "ths = np.linspace(0.46, 0.54, 17)\n",
    "best_thr, best_acc = max(((t, accuracy_score(y, (oof_meta>=t).astype(int))) for t in ths), key=lambda x: x[1])\n",
    "print(f\"STACK OOF calibrated — best_thr: {best_thr:.3f} | acc: {best_acc:.4f}\")\n",
    "\n",
    "# Seuils par groupes (Sex,Pclass) — souvent payant public\n",
    "sex_tr = train_fe.loc[X.index, \"Sex\"].astype(str).values\n",
    "pcl_tr = train_fe.loc[X.index, \"Pclass\"].astype(int).values\n",
    "thr_map = {}\n",
    "for sex in [\"male\",\"female\"]:\n",
    "    for pc in [1,2,3]:\n",
    "        m = (sex_tr==sex)&(pcl_tr==pc)\n",
    "        if m.sum()==0: thr_map[(sex,pc)]=best_thr\n",
    "        else:\n",
    "            t,a = max(((t, accuracy_score(y[m], (oof_meta[m]>=t).astype(int))) for t in ths), key=lambda x:x[1])\n",
    "            thr_map[(sex,pc)] = t\n",
    "            print(f\"group ({sex}, {pc}) thr={t:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "It's time to choose a model, train it on our processed data, and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo-labeled added: 56\n"
     ]
    }
   ],
   "source": [
    "# 4) Fit final + proba test (méta) + PSEUDO-LABELING robuste (fix)\n",
    "\n",
    "# Proba méta sur test (avant pseudo)\n",
    "X_meta_test = scaler_meta.transform(test_stack)\n",
    "test_meta_raw = meta.predict_proba(X_meta_test)[:, 1]\n",
    "test_meta = iso.transform(test_meta_raw)\n",
    "\n",
    "# ---------- Pseudo-labeling ----------\n",
    "# On sélectionne les prédictions test très confiantes\n",
    "high_pos = test_meta >= 0.97\n",
    "high_neg = test_meta <= 0.03\n",
    "\n",
    "pl_mask = high_pos | high_neg\n",
    "pl_test_sel = test_fe.loc[pl_mask].copy()\n",
    "pl_labels   = (test_meta[pl_mask] >= 0.5).astype(int)  # np.ndarray d'entiers (0/1)\n",
    "\n",
    "print(\"Pseudo-labeled added:\", int(pl_test_sel.shape[0]))\n",
    "\n",
    "# Si pas de pseudo-labels -> on garde les proba actuelles et on sort proprement\n",
    "if pl_test_sel.shape[0] == 0:\n",
    "    test_meta2 = test_meta.copy()\n",
    "    test_blend = test_meta2  # pas de blend possible sans refit\n",
    "else:\n",
    "    # Construire un train augmenté puis REFAIRE les TE OOF sur ce train (pas de fuite)\n",
    "    # Important: pas de `.values` ici (pl_labels est déjà un ndarray)\n",
    "    train_aug = pd.concat(\n",
    "        [train_fe, pl_test_sel.assign(Survived=pl_labels.astype(int))],\n",
    "        axis=0, ignore_index=True\n",
    "    )\n",
    "\n",
    "    # --- Fonction pour refaire les TE OOF sur le train augmenté ---\n",
    "    def rebuild_with_te(train_df, test_df):\n",
    "        tr = train_df.copy()\n",
    "        te = test_df.copy()\n",
    "\n",
    "        # TEs simples\n",
    "        for c in [\"FamilyID\", \"TicketClean\", \"Surname\"]:\n",
    "            tr_enc, te_enc = oof_target_encode(tr, te, c, n_splits=5, smoothing=30)\n",
    "            tr[f\"TE_{c}\"] = tr_enc.values\n",
    "            te[f\"TE_{c}\"] = te_enc.values\n",
    "\n",
    "        # TEs composites\n",
    "        for cols in [[\"TicketClean\", \"Sex\"], [\"Surname\", \"Sex\"], [\"FamilyID\", \"Pclass\"]]:\n",
    "            key = \"__\".join(cols)\n",
    "            tr_tmp, te_tmp = tr.copy(), te.copy()\n",
    "            tr_tmp[key] = tr_tmp[cols].astype(str).agg(\"_\".join, axis=1)\n",
    "            te_tmp[key] = te_tmp[cols].astype(str).agg(\"_\".join, axis=1)\n",
    "            tr_enc, te_enc = oof_target_encode(tr_tmp, te_tmp, key, n_splits=5, smoothing=30)\n",
    "            tr[f\"TE_{key}\"] = tr_enc.values\n",
    "            te[f\"TE_{key}\"] = te_enc.values\n",
    "\n",
    "        return tr, te\n",
    "\n",
    "    # Rebuild TE sur train augmenté\n",
    "    train_aug_te, test_te_again = rebuild_with_te(train_aug, test_fe.copy())\n",
    "\n",
    "    # Matrices alignées (mêmes colonnes que X/X_test d'origine)\n",
    "    y_aug = train_aug_te[\"Survived\"].astype(int)\n",
    "    X_aug = train_aug_te[X.columns].copy()\n",
    "    X_test_aug = test_te_again[X_test.columns].copy()\n",
    "\n",
    "    # Refit stacking L1 rapidement sur le train augmenté\n",
    "    oof_stack2 = np.zeros((len(X_aug), len(base_models)))\n",
    "    test_stack2 = np.zeros((len(X_test_aug), len(base_models)))\n",
    "\n",
    "    skf2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=1337)\n",
    "    for m_idx, (name, mdl) in enumerate(base_models):\n",
    "        oof_proba = np.zeros(len(X_aug))\n",
    "        test_f = np.zeros((len(X_test_aug), skf2.get_n_splits()))\n",
    "        for k, (tr, va) in enumerate(skf2.split(X_aug, y_aug)):\n",
    "            pipe = Pipeline([(\"prep\", preprocess), (\"model\", clone(mdl))])\n",
    "            pipe.fit(X_aug.iloc[tr], y_aug.iloc[tr])\n",
    "            oof_proba[va] = pipe.predict_proba(X_aug.iloc[va])[:, 1]\n",
    "            test_f[:, k] = pipe.predict_proba(X_test_aug)[:, 1]\n",
    "        oof_stack2[:, m_idx] = oof_proba\n",
    "        test_stack2[:, m_idx] = test_f.mean(axis=1)\n",
    "\n",
    "    # Méta 2 + calibration\n",
    "    scaler_meta2 = StandardScaler()\n",
    "    X_meta2 = scaler_meta2.fit_transform(oof_stack2)\n",
    "\n",
    "    meta2 = LogisticRegressionCV(Cs=10, cv=5, scoring=\"accuracy\",\n",
    "                                 max_iter=20000, n_jobs=-1, refit=True)\n",
    "    meta2.fit(X_meta2, y_aug)\n",
    "\n",
    "    oof_meta2_raw = meta2.predict_proba(X_meta2)[:, 1]\n",
    "    iso2 = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso2.fit(oof_meta2_raw, y_aug.values)\n",
    "\n",
    "    test_meta2 = iso2.transform(\n",
    "        meta2.predict_proba(scaler_meta2.transform(test_stack2))[:, 1]\n",
    "    )\n",
    "\n",
    "    # Blend prudent (après pseudo + avant pseudo)\n",
    "    test_blend = 0.6 * test_meta2 + 0.4 * test_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Submission File\n",
    "\n",
    "Finally, we'll use our trained model to make predictions on the test set and generate the submission file in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écrit: sub_meta.csv\n",
      "Écrit: submissions.csv\n",
      "Écrit: sub_pseudo.csv\n"
     ]
    }
   ],
   "source": [
    "# 5) Exports\n",
    "\n",
    "# A) seuil global (provenant de l'étape méta 1)\n",
    "predA = (test_meta >= 0.5).astype(int)\n",
    "subA = pd.DataFrame({\"PassengerId\": test_raw[\"PassengerId\"], \"Survived\": predA})\n",
    "subA.to_csv(\"sub_meta.csv\", index=False)\n",
    "print(\"Écrit: sub_meta.csv\")\n",
    "\n",
    "# B) seuils par groupes (Sex,Pclass) appris à l'étape méta 1\n",
    "sex_te = test_fe[\"Sex\"].astype(str).values\n",
    "pcl_te = test_fe[\"Pclass\"].astype(int).values\n",
    "predB = np.zeros(len(test_meta), dtype=int)\n",
    "for i in range(len(test_meta)):\n",
    "    thr = thr_map.get((sex_te[i], int(pcl_te[i])), 0.5)\n",
    "    predB[i] = int(test_meta[i] >= thr)\n",
    "subB = pd.DataFrame({\"PassengerId\": test_raw[\"PassengerId\"], \"Survived\": predB})\n",
    "subB.to_csv(\"submissions.csv\", index=False)\n",
    "print(\"Écrit: submissions.csv\")\n",
    "\n",
    "# C) pseudo-labeling (après augmentation) + léger blend\n",
    "predC = (test_blend >= 0.5).astype(int)\n",
    "subC = pd.DataFrame({\"PassengerId\": test_raw[\"PassengerId\"], \"Survived\": predC})\n",
    "subC.to_csv(\"sub_pseudo.csv\", index=False)\n",
    "print(\"Écrit: sub_pseudo.csv\")\n",
    "\n",
    "# Astuce pratique: soumets d'abord sub_meta.csv et sub_pseudo.csv ; si limite/jour faible, garde le meilleur public et réessaie demain avec sub_group.csv.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
